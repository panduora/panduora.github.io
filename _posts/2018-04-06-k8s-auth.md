---
layout: post
title: Kubernetes 分析之集群认证机制
tags:
- kubernets
categories: kubernetes
---

最近在做 k8s 的部署，因为是直接使用的 VM 进行部署，所以并没有使用各类云的自有方案，而是使用的 kubeadm 进行部署。不得不说 kubeadm 还是做的很便捷的，除去 image 下载问题之外，基本可以通过简单的几条命令便可搭建起一个 k8s 集群。

但是目前的一个缺点是这个 k8s 集群提供的组件并不是高可用的，也就是说搭建一个高可用的 k8s 集群还需要自己做一些工作。在尝试了 kubeadm 出的高可用方案之后，发现 k8s 安全方面的东西对我来说终于不再是谜一样的存在了，本文希望能够把搭建过程中的一些心得体会进行总结和思考。

尝试讲清楚两个问题：

- kubeadm 帮我们做了哪些安全方面的工作？

- k8s 集群包含哪些相关的认证与安全机制？


## 安全相关的一些基础

首先回顾一下我们之前学过的一些安全相关的知识：

- 为了防止 `信息窃听`，我们会使用共同约定的密钥对信息进行加密解密，而为了交换这个 `约定的密钥`，我们引入了公钥私钥机制，A 需要发送信息给 B，则用 B 的公钥进行加密然后发给 B，B 则使用自己的私钥进行解密；

- 为了防止 `信息篡改`，我们又引入了数字签名机制，A 在发送信息给 B 时，会用 A 的私钥对内容 hash 进行加密，B 接到信息后使用 A 的公钥对信息进行确认；

- 为了防止 `冒名顶替`，我们又引入了 CA 机构，B 在接受 A 发送的带有数字签名的消息时，会要求 A 的公钥是经过了 CA 机构认证的；

- CA 机构如何认证，其实就是 CA 机构用自己的私钥对需要认证的公钥及相关信息进行了数字签名，那大家又怎么获得 CA 机构的公钥呢，那就是靠内置！


## kubeadm 启动过程

我们在调用 `kubeadm init` 的过程中，能够看到 kubeadm 的启动过程：

```
[init] Using Kubernetes version: v1.8.0
[init] Using Authorization modes: [Node RBAC]
[preflight] Running pre-flight checks
[kubeadm] WARNING: starting in 1.8, tokens expire after 24 hours by default (if you require a non-expiring token use --token-ttl 0)
[certificates] Generated ca certificate and key.
[certificates] Generated apiserver certificate and key.
[certificates] apiserver serving cert is signed for DNS names [kubeadm-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.138.0.4]
[certificates] Generated apiserver-kubelet-client certificate and key.
[certificates] Generated sa key and public key.
[certificates] Generated front-proxy-ca certificate and key.
[certificates] Generated front-proxy-client certificate and key.
[certificates] Valid certificates and keys now exist in "/etc/kubernetes/pki"
[kubeconfig] Wrote KubeConfig file to disk: "admin.conf"
[kubeconfig] Wrote KubeConfig file to disk: "kubelet.conf"
[kubeconfig] Wrote KubeConfig file to disk: "controller-manager.conf"
[kubeconfig] Wrote KubeConfig file to disk: "scheduler.conf"
[controlplane] Wrote Static Pod manifest for component kube-apiserver to "/etc/kubernetes/manifests/kube-apiserver.yaml"
[controlplane] Wrote Static Pod manifest for component kube-controller-manager to "/etc/kubernetes/manifests/kube-controller-manager.yaml"
[controlplane] Wrote Static Pod manifest for component kube-scheduler to "/etc/kubernetes/manifests/kube-scheduler.yaml"
[etcd] Wrote Static Pod manifest for a local etcd instance to "/etc/kubernetes/manifests/etcd.yaml"
[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory "/etc/kubernetes/manifests"
[init] This often takes around a minute; or longer if the control plane images have to be pulled.
[apiclient] All control plane components are healthy after 39.511972 seconds
[uploadconfig] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[markmaster] Will mark node master as master by adding a label and a taint
[markmaster] Master master tainted and labelled with key/value: node-role.kubernetes.io/master=""
[bootstraptoken] Using token: <token>
[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstraptoken] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[addons] Applied essential addon: kube-dns
[addons] Applied essential addon: kube-proxy

Your Kubernetes master has initialized successfully!
```

主要看 `certificates` 部分的启动步骤，我们能看到 kubeadm 为我们做的事情：

- 生成自建 CA 的私钥文件以及证书文件，注意我们是可以从遵循 X.509 标准的证书中提取出来相关公钥的，所以只要有 CA 的原始证书，我们就能从中提取出来 CA 的公钥而对 CA 所签发的其他证书进行验证；

- 生成 apiserver 私钥，并使用自建 CA 为 apiserver 生成 cert，主要目的是为了使 apiserver 支持 https 访问模式；另外在这中间有一条信息比较关键，即 `apiserver serving cert is signed for DNS names and IPs`。当我们需要支持组件高可用时，由于所有的 apiserver 会共用 cert 文件，所以我们要确保在 CA 为 apiserver 签证书时，就包含了所有高可用机器的 IP；

- 生成 apiserver-kubelet-client 私钥，并使用自建 CA 为其生成 cert；主要作用是在 apiserver 访问 kubelet 时作为 apiserver 的访问凭证，cert 用作身份认证，key 用于进行内容的数字签名；

- 生成 server account 的公私钥文件，私钥会交由 controller-manager 用于创建 service account token，然后由 apiserver 使用公钥对 service account token 进行认证；

- 生成 front-proxy-ca 的私钥及证书文件，当外部使用 proxy 的方式访问 apiserver，并在 request header 中携带相关认证信息时，apiserver 会使用 proxy-ca 的证书文件对请求进行认证；

- 生成 front-proxy-client 的私钥文件，并用 front-proxy-ca 对其进行签名，也就是 apiserver 需要通过 proxy 访问其他地方的时候，使用此 key 与 cert 文件用于自己身份的判断；


除了在控制台上的输出信息外，kubeadm 还为我们做了一些额外的事情：

- 为 kubelet 生成了提供 https 访问所需的 key 以及 cert 文件，存在于 `/var/lib/kubelet/pki` 目录中

- 为 controller-manager、scheduler 生成访问 apiserver 所需 key 以及 cert 文件，信息分别在 `/etc/kubernetes/` controller-manager 以及 scheduler 的相关配置文件里


## k8s 集群的认证与安全机制

在 k8s 各组件的访问中，至少包含以下几点安全相关的事需要考虑：

- apiserver 是集群访问的入口，客户端在访问 apiserver 时如何确定 apiserver 的真实性？

- apiserver 不能让任意用户的调用都放行，那如何验证用户的身份？（这里我们暂且不考虑授权的事情

- controller-manager 会包含需要相关的 controller，如何使得每个 controller 都能够访问 apiserver？

- apiserver 有时需要直接调用 Kubelet，又如何保证这其中的安全性？ 

下面，我们尝试来一一解释这几个问题。

### apiserver 真实性的确定

从前文的背景知识，我们可以得知，这就是一个 `冒名顶替` 的问题，一般我们需要有信任的 CA 机构的证书才能够防止这种欺骗。那解决方式就是我们在客户端中加入 apiserver 当时生成证书所用的 CA 的证书，从而进行验证。所以我们能看到 kubeadm 为我们生成的 kubelet、scheduler、controller-manager 的 config 文件中都包含了相关的 cluster 的 CA data 信息。

另外，kube-proxy 也需要对 apiserver 进行访问，从而得到相关 svc 及 endpoints 信息，其对 apiserver 的认可也与其他组件类似，只不过并不存在落地的相关的 config 文件，而是是 configmap 的形式被 kube-proxy 使用。

不过，对于 apiserver 真实性的判断还是客户端的事，如果说客户端并不介意将数据发送给了一个错误的 server，则可以选择不进行相关认证。所以我们在使用 `kubectl config` 命令的时候，可以选择加入 `insecure-skip-tls-verify: true` 配置，而不去进行相关验证。


### apiserver 如何做 Authentication

apiserver 是支持多种认证方式的，这个在相关文档中都列的比较详细，我们主要讲一下在使用 Kubeadm 启动 k8s 集群时，默认可以使用过的几种认证方式：

- X.509 Client Certs：

    scheduler、controller-manager、kubelet 访问 apiserver 时所使用的认证方式，apiserver 中会设定 `--client-ca-file=/etc/kubernetes/pki/ca.crt`，表示 apiserver 会认可使用这 CA 所签名的 cert，而各组件则会在访问 apiserver 带上 CA 所签发的 cert 文件以及自己的私钥所生成的数字签名；

- Bootstrap tokens

    通过在 apiserver 中设定 `--enable-bootstrap-token-auth=true` 来开启此种认证，kubeadm token 可以生成此类 token，kubeadm 在新加入一个节点时到集群时会使用此 token 请求 apiserver；至于新节点与 master 之间如果进行认证在下一节会进行描述

- Service Account Token

    apiserver 会验证通过 sa.pub 验证的 sa.key 所签发的 token，主要是 controller-manager 会进行使用。在 controller-manager 启动过程中，会设置 `--use-service-account-credentials=true` 参数，使得 controller-manager 为每个 controller 都会签发一个单独的 token，然后 apiserver 会对其进行认证


### kubelet 如何认证其他的调用

apiserver 有时候需要直接调用 kubelet 进行操作，比如查看 log 等等操作，那怎么进行相互认证呢？

对于 master 节点的 kubelet 来说，前文已经提过，apiserver 会使用自己的 kubelet-client 相关 key 和 cert 文件作为访问 kubelet 的凭证；而 kubeadm 也在初始化的时候就为 kubelet 使用 CA 私钥生成了相关 tls 证书以及访问 apiserver 所需的 X.509 相关证书；

对于 minion 节点来说，会相对复杂一点，具体步骤大概如下：

- 前提工作： apiserver  已经存储了 bootstrap token，并且用 bootstrap token 签署了一个 `cluster-info` configMap，此 configMap 存在于 `kube-public` namespace 中

- minion 首先需要确认 apiserver 的真实性：获取上文提到过的 configMap，然后使用 bootstrap token 以及 ca-hash 验证 configMap 中的内容，如果匹配则认为是真正的 apiserver（个人不太理解为啥还需要再进行一次 ca-hash 的认证，虽然文档上说的是防止其他节点冒充 master sever）

- minion 同样通过 bootstrap token 作为自己的认证凭证发送给 apiserver，请求为自己生成的 csr 生成签名信息作为以后的 tls 认证以及 X.509 认证

## References

- https://kubernetes.io/docs/admin/authentication/#x509-client-certs

- https://kubernetes.io/docs/admin/bootstrap-tokens/

- https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/
